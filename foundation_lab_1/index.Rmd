---
  title: "Foundations lab 1 - Data Structures"
  author: "Jeanne McClure, Jennifer Houchins"
  institute: "Instituttional Affiliation"
  date: '`r format(Sys.time(), "%B %d, %Y")`'
  output:
    xaringan::moon_reader:
      css:
       - default
       - css/laser.css
       - css/laser-fonts.css
      lib_dir: libs                        # creates directory for libraries
      seal: false                          # false: custom title slide
      nature:
        highlightStyle: default         # highlighting syntax for code
        highlightLines: true               # true: enables code line highlighting
        highlightLanguage: ["r"]           # languages to highlight
        countIncrementalSlides: false      # false: disables counting of incremental slides
        ratio: "16:9"                      # 4:3 for standard size,16:9
        slideNumberFormat: |
         <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
         </div>
---
class: clear, title-slide, inverse, center, top, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, knitr)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-clipboard, echo=FALSE}
# these allow any code snippets to be copied to the clipboard so they 
# can be pasted easily
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```
```{r xaringan-extras, echo=FALSE}
xaringanExtra::use_tile_view()

```

# `r rmarkdown::metadata$title`
----
### `r rmarkdown::metadata$author`
### `r format(Sys.time(), "%B %d, %Y")`

---
# Recap from Orientation lab

--

- Well Documented
--

- Clear Communication
--

- Reproducible Theory

???

In the orientation lab we learned about reproducible research. Keep your code well documented, use clear communication and follow the reproducible theory.

We also learned about the open movement and its importance for  reproducible research.

From your pre-reading, Krumm 2018 sums it up nicely stating, "The basic idea
behind the open data movement is that anyone can access or use a data set, and key to this movement is not just accessibility but usability. Making open data usable means making it accessible in machine-readable, structured, granular, and well-documented formats."

---

# Agenda

.pull-left[**Part-1  Conceptual Overview**
- Types of Data Used in LA (Sources)
- Characteristics of Data (Format)

]

.pull-right[**Part-2  Code-Along**
- Read in Data
- view Data

.footnote[Pre-Reading:
[Learning Analytics Goes to School, (Ch. 2, pp 28 - 41 ) By Andrew Krumm, Barbara Means, Marie Bienkowski]()
]
]
???
In this session for the conceptual overview we are going to learn more about the types and characteristics of data structures common in Learning Analytics. Many of these you will be familiar with and some may be new.

In our code-along we will continue with learning how to read in data, using the various packages from `tidyverse`. We will also learn to view data between a tibble and a data frame.
---

class: clear, inverse, middle, center

Part 1:

----

Data Structures: Conceptual Overview


---
# Types of Data Used in LA (Sources)


.center[
<img src="img/data_LA.png" height="400px"/>
]


???
Although we are going to talk about these types of data in silo, in practice researchers find value in using them together.

The types of data we are looking at are  Digital Learning Environments,  Administrative data and Sensors and Multi-modal data.

These types of data categories typically show:
- Indicators of a school's success and progress.
- Student progress, student behavior, and student learning and,
- Demographic or social media sources.



---
# Digital Learning Environments

.panelset[

.panel[.panel-name[Definition]

.pull-left[Digital learning environments include any set
of technology-based methods that can be applied to support learning and instruction. Ifenthaler(2017).
]

.pull-right[
<img src="img/digital_LA.png" height="300px"/>
]
]
???



.panel[.panel-name[Games+]

.pull-left[**Games and Simulations:**
- Captures real-time interactions
- Cognitive outcomes
- Behavior outcomes
- Affective outcomes


]

.pull-right[
<img src="img/crystal_island.png" height="300px"/>
]
]

.panel[.panel-name[ITS]


.pull-left[**Intelligent Tutoring Systems**
1. An expert, or domain model
2. A student model
3. An instructional, or pedagogical model

]

.pull-right[
<img src="img/duolingo.png" height="350px"/>
]

]


.panel[.panel-name[LMS]

.pull-left[Learning Management Systems -
"web-based systems that allow instructors and students to share instructional materials, make class announcements, submit and return course assignments, and communicate with each other online." (Krum et al., 2018)

]

.pull-right[
<img src="img/canvas.png" height="300px"/>
]
]

.panel[.panel-name[MOOCs]
.pull-left[
- Free/freemium courses, certificates, PD
- Edx
- Coursera
- Friday Institute
- Collect granular data like ITS
]

.pull-right[
<img src="img/fi_mooc.png" height="300px"/>
]
]
]

???

**Definition tab**
Digital learning environments include any set
of technology-based methods that can be applied to support learning and instruction.

According to Krumm et al, Digital Learning Environments have fueled data intensive research in education more than any other data.

Within the DLE category we include
- Games and Simulations
- Intelligent tutoring systems 
- Learning Management System and,
- MOOCs



**Games tab**
The types of data collected in Games and Simulations is capturing real-time interactions, cognitive outcomes, behavior outcomes and affect outcomes.

These data can be used for 
- Early, iterative user-testing
- help surface basic player interactions for improved core mechanics
- UI/UX and learning design
- Structure discovery and relationship mining

Crystal Island: Uncharted Discovery is an educational video game created by a team of educators and computer scientists at the Center for Educational Informatics aimed at teaching students upper elementary science education, focusing on landforms, navigation, and modeling. In the game, students play as shipwrecked adventurers on a volcanic island. To escape the island, they must complete a series of quests that test their critical thinking skills and teach them content-related information. Upon completion of the quests, the players gain access to a new area of the island which contains a multi-skill quest that requires students to use the knowledge and skills they learned during the previous quests. Once the final quest is completed, the players gain access to a communication device which allows them to call the outside world for rescue.

Crystal Island fuses automated reasoning and human language technologies with game technologies and new media to investigate high-impact adaptive learning environments.




**intelligent tutoring tab**

Intelligent Tutoring Systems(ITS for short) is a type of DLE that applies AI to students interactions in the digital system.

ITS includes three main models, 
1. an expert, or domain model, which organizes the skills and strategies in the domain, 
2. a student model of what a student understands about the domain that is inferred from their performances on learning tasks and,
3. an instructional (pedagogical model) that includes  common mistakes and misconceptions along with a corresponding feedback strategy 

Intelligent tutoring systems are online based teaching that can generate student feedback with no human interaction.

The same data that the ITS uses to figure out how to respond to a student’s actions can also be used by human analysts to gain a detailed picture of the learning processes and the behaviors learners engage in.

Duolingo app uses artificial intelligence known as spaced repetition. This concept delivers personalized language lessons over longer intervals for optimal learning rather than cramming lessons into a short period of time.




**LMS tab**
We are all probably pretty familiar with Learning Management Systems(LMS).
A Learning Management System (LMS) is an online integrated software used for creating, delivering, tracking, and reporting educational courses and outcomes. 

Contributes to trace data, survey data, discussion posts, open ended questions and video diaries.
Tools allow instructors to provide feedback to students’

Often used as *student-facing repositories* of digital resources and activities.

At North Carolina State University we use Moodle. What LMS do you use at your institution? What types of data does it generate?





**MOOCs tab**
Raise your hand if you have ever participated in a MOOC? The FI has many free MOOCs available, but there are also EDx and Coursera that you can take a MOOC at. These offer a major source for data from the large numbers of participants.

Friday Insitutes free Moocs offer professional Development for in-service and pre-service K12 educators. For more information check out the website include in the speaker notes. https://place.fi.ncsu.edu/

Data from MOOCs can take a closer look at how participants are interacting within the classroom social network.

Have you ever pulled data from your own MOOC? or learning management system? 

What types of insights were analyzed?

---
# Administrative Data

.panelset[

.panel[.panel-name[Definition]

.pull-left[Types of State and Federal systems used in schools and districts to collect and store information to deliver some *sort* of service.]

.pull-right[
<img src="img/Admin_LA.png" height="300px"/>
]
]
???
Administrative Data is the second type of data fueling data intensive research.
Used in Schools, at district level and throughout most states.

There are two main types 
- Student Information System or SIS
- Statewide longitudinal data systems

.panel[.panel-name[SIS]

.pull-left[Student Information Systems:
- Student level - Academic Performance
- Teacher & Administrator facing repository
- Technology-based intervention

]

.pull-right[
<img src="img/powerschool.png" height="300px"/>
]
]
???
**Student information systems**
Can be at the Student level or Teacher and Administrator level of student demographic and learning-outcome data.

- A ready source of data on educational outcomes (e.g., grades) and demographic information, which can play a role in evaluations of technology-based interventions as well as early warning system research 


.panel[.panel-name[SLDS]

.pull-left[
- Connects state-wide
- Accountability and reporting

]

.pull-right[
<img src="img/slds.png" height="300px"/>
]
]
???
**statewide longitudinal data systems** are a continuum of data that includes standardized test performances, attendance, and major behavioral infractions.


Which carries a unique statewide identifier for every student.

The SLDS has the ability to link state K12 with HEd system includes the storage of each student’s demographic characteristics and enrollment history. Includes all scores on state accountability tests

Krumm et al., additionally mentions the importance of the data infrastructure to examine “educational outcomes at the scale of an entire state” p.34

State  level  and  university-based  researchers  increasingly  leveraged  these  data  for  both  accountability  and  reporting  purposes  as  well  as  district-and  school-improvement purposes. Typically they are not available to the public rather threw an IRB and MOA through DPI. (Lots of acronyms)

North Carolina public education state longitudinal data system (SLDS) as presented through publicly available resources of public primary, secondary and higher education, information made available to the public through the National Center for Education Statistics (NCES).

https://childandfamilypolicy.duke.edu/north-carolina-education-research-data/

Or a free version through EPIC at UNC. 

https://childandfamilypolicy.duke.edu/north-carolina-education-research-data/

]

---
# Sensors and Multimodal

.panelset[

.panel[.panel-name[Sensors and recording devices]

.pull-left[
- Location 
- Physical movement 
- Speech data
]

.pull-right[
- Capture biometric data
- Parse audio  
- Video recordings using machine learning and artificial intelligence techniques.
]
]
???
How many of you have an apple watch, or a fitbit or Samsung? These watches measure different metrics surrounding our health and fitness. The data can then be used to explain or predict our **health**

Senors and video recordings collect data by capturing biometric data, parse audio and from video recordings using machine learning and artificial intelligence techniques.

.panel[.panel-name[Multimodal]

- Identifying multiple affective 
- Engagement-related states
- Detect facial expressions
- Speech recognition
- Speech prosody


]
???
Krumm et al. suggest that multimodal  investigation identify multiple affective and engagement-related states, detect facial expressions and use audio, and video, for speech recognition and speech prosody.

- Understanding  what  learners  do  as  they  engage  in  learning  tasks  can  drive  digital  learning  environment adaptations.


]
---
# Characteristics of Data

.panelset[

.panel[.panel-name[Types]


.center[
<img src="img/data_types.png" height="400px"/>
]
]

???

In data-intensive research we might acquire data from LMS, SIS or another source. Data maybe to big for a typical  database  software  tools  to  capture,  store,  manage,  and  analyze”

Those data are similar but different because 
1. the tasks that students are engaged in and 
2. how data from those tasks are collected and stored by the technology. 

We identify four main types of data structures.
- structured
- unstructured
- semi-structured and 
- meta-data



.panel[.panel-name[Structured]

```{r round-block, echo=FALSE, warning=FALSE, message=FALSE}
#show first 5 rows
library(tidyverse)
sci_course <- read_csv("data/sci-online-classes.csv")

knitr::kable(head(sci_course), format = 'html')
```

]
???
Structured data is Quantitative in nature
Here we see the rows and columns nice and tidy. 
- data is tidy when each column is a variable, each row is an observation, and each cell is a single value.

We typically have tidy data in Excel files or SQL databases that we can import using the readr function or here function.

- Tabular - relational data. 
Examples might include:
- performance data (current and prior)
- demographic data (student’s ethnic and or group or disability)

Each field (variable) is discrete and can be accessed separately or jointly along with data from other fields (variables).

.panel[.panel-name[Unstructured]

.center[
<img src="img/unstructured.png" height="400px"/>
]
]
???
Unstructured data is Qualitative in nature
and includes:
- Non-Relational Data
- sensor data, web logs, clickstreams, keystroke capture data, emails, images and videos

Text Heavy relationships between nodes
IE: Audio, video, dates SQL database

.panel[.panel-name[Semi-Structured]

.center[
<img src="img/semi.png" height="400px"/>          
]
]
???

Semi-structured data includes Qualitative 
Triangular Data (uses surveys and interview answers)
Semi-structured data is a form of structured data that does not conform with the formal structure of data models associated with relational databases or other forms of data tables.

Semi structured can also include log data.

It is a self-describing structure that contains tags
examples: Social media, JSON and XML, Social Networkdata

.panel[.panel-name[Meta-Data]

.center[
<img src="img/meta_dat.png" height="300px"/>
]

]
???
Not in the readings but something to think about is Meta-data collection, such as the field dates, times and locations of photos, videos and other structured data about data.
 - contexts and purposes for why the data was collected
 
You can read more about the meta data standards on Canada's heritage information network. The link is in the presenters notes. https://www.canada.ca/en/heritage-information-network/services/collections-documentation-standards/chin-guide-museum-standards/metadata-data-structure.html


]

---

class: clear, inverse, middle, center

Part 2:

----

Code-Along

---
# Set Working Directory

```{r eval = FALSE, echo=TRUE}
#sets your working directory
setwd("/path/to/my/directory")

# tells you what working directory you are in
getwd()
```

???
Although we are using projects we want to make sure you are aware of setting the working directory.

Continuing on from the orientation code-along we will use "good programming practices" to learn more about reading in data.

Again we are using a script file in R Studio. You will become familiar with R Markdown files in the next foundation lab. 

A default working directory is a folder where RStudio goes, every time you open it. You can change the default working directory from RStudio menu under: Tools –> Global options –> click on “Browse” to select the default working directory you want.

Although you can set your working directory in Session menu tool it is better to set it with code for reproducibility using the setwd() function.

You can also create a project and connect to your github account. Then open up the project which also sets the directory.

---
# Read in Data
.panelset[

.panel[.panel-name[package]

```{r echo=TRUE}
# Load tidyverse package 
library(tidyverse)#<<

# could load only readr 
library(readr) #<<

```
]
???

In order to read in data we must load packages that allow us to read data in, like we did in foundation lab 1. 

Although there are read-in capabilities with the `utils package` in base r, we are going to use a more modern approach.

One of the packages in Tidyverse allows for reading in Comma delimiter files, or CSV. 


*Reminder if needed - this was discussed in foundation 1*
R comes with several packages in base r when you start a session. Packages are a collection of objects, data sets around a theme or purpose. You may find a package whose objects are for the purpose of correlation, like in the cor package. Or you may find a package that has objects to produce better tables, like in the *kable extra* package. Or you may find a package that will help wrangling the data (processing it) easier than in Base R package. That is the package we are going to install here Tidyverse.

Tidyverse is a suite of packages that share a common philosophy and are designed to work together. Think Google products or Microsoft products. data importing, data wrangling and plotting.

.panel[.panel-name[functions]
Common readr functions

<img src="img/readr_function.png" height="300px"/>

]
???

Common functions that are included when you read in data are 
1. the file, we need to give a path to get the data. It should be in the working directory.

2. column names corresponds to variables on the first row. The first row of the input will be used as the column names, and will not be included in the data frame. If FALSE, column names will be generated automatically: X1, X2, X3 etc.

3. NA corresponds to missing values. commonly you will see capital NA but you may also see quotations or only a period depending on the original database (excel, SAS, STATA etc).

4. Skip functions allows you to skill to when your data starts. 

5. col_types is not typically needed but good to know. R guesses what type of column types you have when it reads in the file using the default guess_max.
This is convenient (and fast), but not robust. If the imputation fails, you'll need to increase the guess_max or supply the correct types yourself.

6. guess_max if the imputation fails increase the number beyond the 1000 default to guess col_types correctly.

.panel[.panel-name[readr]
# Using the readr function

Learn more about [readr.](https://readr.tidyverse.org/)

Use `read_csv()` function to read in CSV.

.pull-left[
```{r echo = TRUE, message = FALSE, warning=FALSE}
online_classes <- read_csv("data/sci-online-classes.csv")

head(online_classes, n=5)

```
]

.pull-right[
<img src="img/delimiter.png" height="300px"/>
]
]
???
readr is a quick way to read in comma-separated values (CSV) and tab-separated values (TSV)

- We read in from the path. My data is stored in a folder called data. I then use the assignment operator to save it as a new object called online-classes. 
- (instructor is showing how to import from file) a quick way to see where your data file is stored is by clicking on the file under files and then import. 


.panel[.panel-name[readxl]
# read in excel files

Read in with `readxl` package

```{r echo=TRUE, warning=FALSE, message=FALSE}
library(readxl) #<<
csss_tweets <- read_excel("data/csss_tweets.xlsx")
head(csss_tweets, n = 5)
```
]

???
To read in an excel file we will need to install the readxl package and then read in with the read_excel function. We can see that it is an excel file with the .xlsx extension.

.panel[.panel-name[functions]

Check what sheets are included.

```{r echo = TRUE, message = FALSE, warning=FALSE}
?read_excel

excel_sheets("data/csss_tweets.xlsx") # <<
csss_tweets <- read_excel("data/csss_tweets.xlsx", sheet = "Sheet1")
head(csss_tweets, n = 5)

```

]
???
You can check what sheets are included in your excel file by calling it with the excel_sheets function. 
Then you can call only that sheet. Our data only includes one sheet so we do not really need to use this function, but it's good to know.
]
---
# Read in data cont.
.panelset[

.panel[.panel-name[path]

Read in from Path

```{r echo = TRUE, message = FALSE, warning = FALSE}
sci_ol_classes <- read_csv("C:/Users/User/Documents/RProj22/foundation_labs_2022/foundation_lab_1/data/sci-online-classes.csv") #<<
```


]
???

You can read in from your computers path. If you aren't sure you go to the file and hold down Shift on your keyboard and right-click on the file.
You would need to change the direction of the slashes so that R can read it in.

.panel[.panel-name[url]

Read in a url path with `read_csv2` function.

```{r echo = TRUE, message = FALSE, warning = FALSE}
air_quality <- read_csv2("https://www4.stat.ncsu.edu/~online/datasets/AirQualityUCI.csv") #<<
head(air_quality, n=3)
```


]
???

If you want to read in a url you use the read_csv2 function. 

.panel[.panel-name[.dta file]

Read in a Stata file with `read_dta` function

```{r echo = TRUE}
library(haven)

gpa_dt <- read_dta("data/GPA3.dta") #<<
```

]

]


---

class: inverse, clear, center

## .font130[.center[**Thank you!**]]

.left-col[
.center[<img style="border-radius: 50%;" src="img/author1.jpeg" height="120px"/><br/>**First Author**<br/><mailto:author1@email.com>]
]
.center-col[
.center[<img style="border-radius: 50%;" src="img/author2.jpeg" height="120px"/><br/>**Second Author**<br/><mailto:author2@email.com>]
]
.right-col[
.center[<img style="border-radius: 50%;" src="img/author3.jpeg" height="120px"/><br/>**Third Author**<br/><mailto:author3@email.com>]
]
<br/><br/><br/><br/><br/>

.pull-left-narrow[<br/>.center[<img style="border-radius: 50%;" src="https://www.nsf.gov/images/logos/NSF_4-Color_bitmap_Logo.png" height="100px"/> ]]

.pull-right-wide[
.left[.font70[
This work was supported by countless cups of hot tea, cookies, and the RStats Xaringan community willingness to post of plethora of tutorials. But you may wish to thank your funding agency (e.g., the National Science Foundation) and put their logo on the left as shown here.
]]
]
---
<img src="img/team_2022.png" height="550px"/>
